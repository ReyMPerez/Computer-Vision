{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHgsO1kTJefv"
      },
      "source": [
        "# PPO for Lunar Lander with Stable Baselines3\n",
        "\n",
        "This notebook implements Proximal Policy Optimization (PPO) to solve the Lunar Lander environment from OpenAI Gymnasium."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TiAKIp4Jefw"
      },
      "source": [
        "## 1. Install Required Packages\n",
        "\n",
        "Run this cell first to install all dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB0HiTqVJefw"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install tensorboard\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "import os\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the environment\n",
        "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
        "\n",
        "# Reset the environment to generate the first observation\n",
        "observation, info = env.reset(seed=42)\n",
        "for _ in range(1000):\n",
        "    # this is where you would insert your policy\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # step (transition) through the environment with the action\n",
        "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # If the episode has ended then we can reset to start a new episode\n",
        "    if terminated or truncated:\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "kLDRq0vPMohk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-5ACe6DJefx"
      },
      "source": [
        "## 3. Create the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCevgmH5Jefy"
      },
      "outputs": [],
      "source": [
        "# Create a single environment for testing\n",
        "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
        "obs, info = env.reset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBsI-mu1Jefy"
      },
      "source": [
        "## 4. Create Vectorized Environment for Training\n",
        "\n",
        "Using multiple parallel environments speeds up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIfGAZK0Jefy"
      },
      "outputs": [],
      "source": [
        "# Create vectorized environment with 4 parallel environments\n",
        "vec_env = make_vec_env('LunarLander-v3', n_envs=10, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzJa3XktJefy"
      },
      "source": [
        "## 5. Initialize PPO Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rkQ3Rv5Jefy"
      },
      "outputs": [],
      "source": [
        "# Create directories for logs and models\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Initialize PPO agent with custom hyperparameters\n",
        "model = PPO(\n",
        "    \"MlpPolicy\", env, learning_rate=0.0003, n_steps=2048, batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95, clip_range=0.2, clip_range_vf=None, normalize_advantage=True, ent_coef=0.0, vf_coef=0.5, max_grad_norm=0.5, use_sde=False, sde_sample_freq=-1, rollout_buffer_class=None, rollout_buffer_kwargs=None, target_kl=None, stats_window_size=100, tensorboard_log=None, policy_kwargs=None, verbose=0, seed=None, device='auto', _init_setup_model=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6csiQbnAJefy"
      },
      "source": [
        "## 6. Set Up Callbacks for Evaluation and Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vashByhkJefy"
      },
      "outputs": [],
      "source": [
        "# Create evaluation environment\n",
        "eval_env = gym.make('LunarLander-v3')\n",
        "\n",
        "# Evaluation callback - evaluates the model every 10000 steps\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=\"./models/\",\n",
        "    log_path=\"./logs/\",\n",
        "    eval_freq=10000,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Checkpoint callback - saves the model every 50000 steps\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=50000,\n",
        "    save_path=\"./models/\",\n",
        "    name_prefix=\"ppo_model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20OoBhXLJefy"
      },
      "source": [
        "## 7. Train the Agent\n",
        "\n",
        "This will train for 500,000 timesteps. Adjust as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsCo-gqFJefz"
      },
      "outputs": [],
      "source": [
        "# Train the agent\n",
        "total_timesteps = 500000\n",
        "\n",
        "print(f\"Starting training for {total_timesteps} timesteps...\")\n",
        "model.learn(\n",
        "    total_timesteps=total_timesteps,\n",
        "    callback=[eval_callback, checkpoint_callback],\n",
        "    progress_bar=False\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "\n",
        "# Save the final model\n",
        "model.save(\"models/ppo_lunar_lander_final\")\n",
        "print(\"Final model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFPDiBQEJefz"
      },
      "source": [
        "## 8. Evaluate the Trained Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Dn3D3XAJefz"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model = PPO.load(\"models/ppo_lunar_lander_final.zip\")\n",
        "\n",
        "# Evaluate the agent\n",
        "eval_env = gym.make('LunarLander-v3')\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"Mean reward over 100 episodes: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
        "\n",
        "eval_env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFExxWK6Qhbk"
      },
      "outputs": [],
      "source": [
        "def create_episode_animation(model, env_name='LunarLander-v3', deterministic=True):\n",
        "    \"\"\"\n",
        "    Create an animated visualization of one episode.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "\n",
        "    # Collect frames from one episode\n",
        "    frames = []\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    step_count = 0\n",
        "\n",
        "    while not done:\n",
        "        # Render and store frame\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        # Get action from trained model\n",
        "        action, _ = model.predict(obs, deterministic=deterministic)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "        step_count += 1\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    print(f\"Episode completed in {step_count} steps with total reward: {total_reward:.2f}\")\n",
        "\n",
        "    # Create animation\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.axis('off')\n",
        "    img = ax.imshow(frames[0])\n",
        "\n",
        "    def animate(i):\n",
        "        img.set_array(frames[i])\n",
        "        ax.set_title(f'Step: {i+1}/{len(frames)} | Reward: {total_reward:.2f}', fontsize=14)\n",
        "        return [img]\n",
        "\n",
        "    anim = animation.FuncAnimation(\n",
        "        fig, animate, frames=len(frames), interval=50, blit=True, repeat=True\n",
        "    )\n",
        "\n",
        "    plt.close()  # Prevent static display\n",
        "    return anim\n",
        "\n",
        "# Create and display the animation\n",
        "print(\"Creating animation of trained agent...\\n\")\n",
        "anim = create_episode_animation(model)\n",
        "HTML(anim.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAAzIHf4Jefz"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Setting up the Lunar Lander environment\n",
        "2. Training a PPO agent with Stable Baselines3\n",
        "3. Evaluating the trained agent\n",
        "4. Visualizing performance\n",
        "5. Comparing with a random baseline\n",
        "\n",
        "The PPO algorithm should achieve an average reward above 200 (considered solved) after sufficient training."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ghW4ZBpxeqaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
